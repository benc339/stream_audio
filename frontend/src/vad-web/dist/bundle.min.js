!function(e,t){"object"==typeof exports&&"object"==typeof module?module.exports=t(require("onnxruntime-web")):"function"==typeof define&&define.amd?define(["onnxruntime-web"],t):"object"==typeof exports?exports.vad=t(require("onnxruntime-web")):e.vad=t(e.ort)}(self,(e=>(()=>{"use strict";var t={917:t=>{t.exports=e}},s={};function i(e){var o=s[e];if(void 0!==o)return o.exports;var r=s[e]={exports:{}};return t[e](r,r.exports,i),r.exports}i.d=(e,t)=>{for(var s in t)i.o(t,s)&&!i.o(e,s)&&Object.defineProperty(e,s,{enumerable:!0,get:t[s]})},i.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),i.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})};var o={};return(()=>{i.r(o),i.d(o,{AudioNodeVAD:()=>b,FrameProcessor:()=>d,Message:()=>e,MicVAD:()=>A,NonRealTimeVAD:()=>T,defaultNonRealTimeVADOptions:()=>P,defaultRealTimeVADOptions:()=>y,utils:()=>R});var e,t=i(917);function s(e,t,s){for(var i=0;i<s.length;i++)e.setUint8(t+i,s.charCodeAt(i))}!function(e){e.AudioFrame="AUDIO_FRAME",e.SpeechStart="SPEECH_START",e.VADMisfire="VAD_MISFIRE",e.SpeechEnd="SPEECH_END",e.SpeechStop="SPEECH_STOP",e.AudioData="audio-data"}(e||(e={}));const r=["error","debug","warn"].reduce(((e,t)=>(e[t]=function(e){return(...t)=>{console[e]("[VAD]",...t)}}(t),e)),{}),n=[512,1024,1536],a={positiveSpeechThreshold:.5,negativeSpeechThreshold:.35,preSpeechPadFrames:1,redemptionFrames:8,frameSamples:1536,minSpeechFrames:3,submitUserSpeechOnPause:!1};function h(e){n.includes(e.frameSamples)||r.warn("You are using an unusual frame size"),(e.positiveSpeechThreshold<0||e.negativeSpeechThreshold>1)&&r.error("postiveSpeechThreshold should be a number between 0 and 1"),(e.negativeSpeechThreshold<0||e.negativeSpeechThreshold>e.positiveSpeechThreshold)&&r.error("negativeSpeechThreshold should be between 0 and postiveSpeechThreshold"),e.preSpeechPadFrames<0&&r.error("preSpeechPadFrames should be positive"),e.redemptionFrames<0&&r.error("preSpeechPadFrames should be positive")}const c=e=>{const t=e.reduce(((e,t)=>(e.push(e.at(-1)+t.length),e)),[0]),s=new Float32Array(t.at(-1));return e.forEach(((e,i)=>{const o=t[i];s.set(e,o)})),s};class d{constructor(t,s,i){this.modelProcessFunc=t,this.modelResetFunc=s,this.options=i,this.speaking=!1,this.redemptionCounter=0,this.active=!1,this.reset=()=>{this.speaking=!1,this.audioBuffer=[],this.modelResetFunc(),this.redemptionCounter=0},this.pause=()=>(this.active=!1,this.options.submitUserSpeechOnPause?this.endSegment():(this.reset(),{})),this.resume=()=>{this.active=!0},this.endSegment=()=>{const t=this.audioBuffer;this.audioBuffer=[];const s=this.speaking;this.reset();const i=t.reduce(((e,t)=>e+ +t.isSpeech),0);if(s){if(i>=this.options.minSpeechFrames){const s=c(t.map((e=>e.frame)));return{msg:e.SpeechEnd,audio:s}}return{msg:e.VADMisfire}}return{}},this.process=async t=>{if(!this.active)return{};const s=await this.modelProcessFunc(t);if(this.audioBuffer.push({frame:t,isSpeech:s.isSpeech>=this.options.positiveSpeechThreshold}),s.isSpeech>=this.options.positiveSpeechThreshold&&this.redemptionCounter&&(this.redemptionCounter=0),s.isSpeech>=this.options.positiveSpeechThreshold&&!this.speaking)return this.speaking=!0,{probs:s,msg:e.SpeechStart};if(s.isSpeech<this.options.negativeSpeechThreshold&&this.speaking&&++this.redemptionCounter>=this.options.redemptionFrames){this.redemptionCounter=0,this.speaking=!1;const t=this.audioBuffer;if(this.audioBuffer=[],t.reduce(((e,t)=>e+ +t.isSpeech),0)>=this.options.minSpeechFrames){const i=c(t.map((e=>e.frame)));return{probs:s,msg:e.SpeechEnd,audio:i}}return{probs:s,msg:e.VADMisfire}}if(!this.speaking)for(;this.audioBuffer.length>this.options.preSpeechPadFrames;)this.audioBuffer.shift();return{probs:s}},this.audioBuffer=[],this.reset()}}class p{constructor(e,t){this.ort=e,this.modelFetcher=t,this.init=async()=>{r.debug("initializing vad");const e=await this.modelFetcher();this._session=await this.ort.InferenceSession.create(e),this._sr=new this.ort.Tensor("int64",[16000n]),this.reset_state(),r.debug("vad is initialized")},this.reset_state=()=>{const e=Array(128).fill(0);this._h=new this.ort.Tensor("float32",e,[2,1,64]),this._c=new this.ort.Tensor("float32",e,[2,1,64])},this.process=async e=>{const t={input:new this.ort.Tensor("float32",e,[1,e.length]),h:this._h,c:this._c,sr:this._sr},s=await this._session.run(t);this._h=s.hn,this._c=s.cn;const[i]=s.output.data;return{notSpeech:1-i,isSpeech:i}}}}p.new=async(e,t)=>{const s=new p(e,t);return await s.init(),s};class u{constructor(e){this.options=e,this.process=e=>{const t=[];for(const t of e)this.inputBuffer.push(t);for(;this.inputBuffer.length*this.options.targetSampleRate/this.options.nativeSampleRate>this.options.targetFrameSize;){const e=new Float32Array(this.options.targetFrameSize);let s=0,i=0;for(;s<this.options.targetFrameSize;){let t=0,o=0;for(;i<Math.min(this.inputBuffer.length,(s+1)*this.options.nativeSampleRate/this.options.targetSampleRate);)t+=this.inputBuffer[i],o++,i++;e[s]=t/o,s++}this.inputBuffer=this.inputBuffer.slice(i),t.push(e)}return t},e.nativeSampleRate<16e3&&r.error("nativeSampleRate is too low. Should have 16000 = targetSampleRate <= nativeSampleRate"),this.inputBuffer=[]}}const l={...a,ortConfig:void 0};class m{static async _new(e,t,s={}){const i={...l,...s};void 0!==i.ortConfig&&i.ortConfig(t);const o=new this(e,t,i);return await o.init(),o}constructor(t,s,i){this.modelFetcher=t,this.ort=s,this.options=i,this.init=async()=>{const e=await p.new(this.ort,this.modelFetcher);this.frameProcessor=new d(e.process,e.reset_state,{frameSamples:this.options.frameSamples,positiveSpeechThreshold:this.options.positiveSpeechThreshold,negativeSpeechThreshold:this.options.negativeSpeechThreshold,redemptionFrames:this.options.redemptionFrames,preSpeechPadFrames:this.options.preSpeechPadFrames,minSpeechFrames:this.options.minSpeechFrames,submitUserSpeechOnPause:this.options.submitUserSpeechOnPause}),this.frameProcessor.resume()},this.run=async function*(t,s){const i={nativeSampleRate:s,targetSampleRate:16e3,targetFrameSize:this.options.frameSamples},o=new u(i).process(t);let r,n;for(const t of[...Array(o.length)].keys()){const s=o[t],{msg:i,audio:a}=await this.frameProcessor.process(s);switch(i){case e.SpeechStart:r=t*this.options.frameSamples/16;break;case e.SpeechEnd:n=(t+1)*this.options.frameSamples/16,yield{audio:a,start:r,end:n}}}const{msg:a,audio:h}=this.frameProcessor.endSegment();a==e.SpeechEnd&&(yield{audio:h,start:r,end:o.length*this.options.frameSamples/16})},h(i)}}const f={minFramesForTargetMS:function(e,t,s=16e3){return Math.ceil(e*s/1e3/t)},arrayBufferToBase64:function(e){for(var t="",s=new Uint8Array(e),i=s.byteLength,o=0;o<i;o++)t+=String.fromCharCode(s[o]);return btoa(t)},encodeWAV:function(e,t=3,i=16e3,o=1,r=32){var n=r/8,a=o*n,h=new ArrayBuffer(44+e.length*n),c=new DataView(h);return s(c,0,"RIFF"),c.setUint32(4,36+e.length*n,!0),s(c,8,"WAVE"),s(c,12,"fmt "),c.setUint32(16,16,!0),c.setUint16(20,t,!0),c.setUint16(22,o,!0),c.setUint32(24,i,!0),c.setUint32(28,i*a,!0),c.setUint16(32,a,!0),c.setUint16(34,r,!0),s(c,36,"data"),c.setUint32(40,e.length*n,!0),1===t?function(e,t,s){for(var i=0;i<s.length;i++,t+=2){var o=Math.max(-1,Math.min(1,s[i]));e.setInt16(t,o<0?32768*o:32767*o,!0)}}(c,44,e):function(e,t,s){for(var i=0;i<s.length;i++,t+=4)e.setFloat32(t,s[i],!0)}(c,44,e),h}},S=e=>fetch(e).then((e=>e.arrayBuffer())),g="undefined"!=typeof window&&void 0!==window.document?window.document.currentScript:null;let w="/";g&&(w=g.src.replace(/#.*$/,"").replace(/\?.*$/,"").replace(/\/[^\/]+$/,"/"));const v=e=>w+e,F=t,y={...a,onFrameProcessed:e=>{},onVADMisfire:()=>{r.debug("VAD misfire")},onSpeechStart:()=>{r.debug("Detected speech start")},onSpeechEnd:()=>{r.debug("Detected speech end")},onAudioFrame:e=>{},workletURL:v("vad.worklet.bundle.min.js"),modelURL:v("silero_vad.onnx"),modelFetcher:S,stream:void 0,ortConfig:void 0};class A{static async new(e={}){const t={...y,...e};let s;h(t),s=void 0===t.stream?await navigator.mediaDevices.getUserMedia({audio:{...t.additionalAudioConstraints,channelCount:1,echoCancellation:!0,autoGainControl:!0,noiseSuppression:!0,...t.additionalAudioConstraints?t.additionalAudioConstraints:{}}}):t.stream;const i=new AudioContext,o=new MediaStreamAudioSourceNode(i,{mediaStream:s}),r=await b.new(i,t);return r.receive(o),new A(t,i,s,r,o)}constructor(e,t,s,i,o,r=!1){this.options=e,this.audioContext=t,this.stream=s,this.audioNodeVAD=i,this.sourceNode=o,this.listening=r,this.pause=()=>{this.audioNodeVAD.pause(),this.listening=!1},this.start=()=>{this.audioNodeVAD.start(),this.listening=!0},this.destroy=()=>{this.listening&&this.pause(),void 0===this.options.stream&&this.stream.getTracks().forEach((e=>e.stop())),this.sourceNode.disconnect(),this.audioNodeVAD.destroy(),this.audioContext.close()}}}class b{static async new(t,s={}){const i={...y,...s};h(i),void 0!==i.ortConfig&&i.ortConfig(F);try{await t.audioWorklet.addModule(i.workletURL)}catch(e){throw console.error(`Encountered an error while loading worklet. Please make sure the worklet vad.bundle.min.js included with @ricky0123/vad-web is available at the specified path:\n        ${i.workletURL}\n        If need be, you can customize the worklet file location using the \`workletURL\` option.`),e}const o=new AudioWorkletNode(t,"vad-helper-worklet",{processorOptions:{frameSamples:i.frameSamples}});let r;try{r=await p.new(F,(()=>i.modelFetcher(i.modelURL)))}catch(e){throw console.error(`Encountered an error while loading model file. Please make sure silero_vad.onnx, included with @ricky0123/vad-web, is available at the specified path:\n      ${i.modelURL}\n      If need be, you can customize the model file location using the \`modelsURL\` option.`),e}const n=new d(r.process,r.reset_state,{frameSamples:i.frameSamples,positiveSpeechThreshold:i.positiveSpeechThreshold,negativeSpeechThreshold:i.negativeSpeechThreshold,redemptionFrames:i.redemptionFrames,preSpeechPadFrames:i.preSpeechPadFrames,minSpeechFrames:i.minSpeechFrames,submitUserSpeechOnPause:i.submitUserSpeechOnPause}),a=new b(t,i,n,o);return o.port.onmessage=async t=>{if(t.data?.message===e.AudioFrame){const e=t.data.data,s=new Float32Array(e);await a.processFrame(s)}},a}constructor(t,s,i,o){this.ctx=t,this.options=s,this.frameProcessor=i,this.entryNode=o,this.pause=()=>{const e=this.frameProcessor.pause();this.handleFrameProcessorEvent(e)},this.start=()=>{this.frameProcessor.resume()},this.receive=e=>{e.connect(this.entryNode)},this.processFrame=async e=>{const t=await this.frameProcessor.process(e);this.handleFrameProcessorEvent(t),this.isSpeechActive&&this.options.onAudioFrame&&this.options.onAudioFrame(e)},this.handleFrameProcessorEvent=t=>{switch(void 0!==t.probs&&this.options.onFrameProcessed(t.probs),t.msg){case e.SpeechStart:this.isSpeechActive=!0,this.options.onSpeechStart();break;case e.VADMisfire:this.options.onVADMisfire();break;case e.SpeechEnd:this.isSpeechActive=!1,this.options.onSpeechEnd(t.audio)}},this.destroy=()=>{this.entryNode.port.postMessage({message:e.SpeechStop}),this.entryNode.disconnect()},this.isSpeechActive=!1}}const P={modelURL:v("silero_vad.onnx"),modelFetcher:S};class T extends m{static async new(e={}){const{modelURL:s,modelFetcher:i}={...P,...e};return await this._new((()=>i(s)),t,e)}}const R={audioFileToArray:async function(e){const t=new OfflineAudioContext(1,1,44100),s=new FileReader;let i=null;if(await new Promise((o=>{s.addEventListener("loadend",(e=>{const r=s.result;t.decodeAudioData(r,(e=>{i=e,t.startRendering().then((e=>{console.log("Rendering completed successfully"),o()})).catch((e=>{console.error(`Rendering failed: ${e}`)}))}),(e=>{console.log(`Error with decoding audio data: ${e}`)}))})),s.readAsArrayBuffer(e)})),null===i)throw Error("some shit");let o=i,r=new Float32Array(o.length);for(let e=0;e<o.length;e++)for(let t=0;t<o.numberOfChannels;t++)r[e]+=o.getChannelData(t)[e];return{audio:r,sampleRate:o.sampleRate}},...f}})(),o})()));