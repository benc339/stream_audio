!function(e,t){"object"==typeof exports&&"object"==typeof module?module.exports=t(require("onnxruntime-web")):"function"==typeof define&&define.amd?define(["onnxruntime-web"],t):"object"==typeof exports?exports.vad=t(require("onnxruntime-web")):e.vad=t(e.ort)}(self,(e=>(()=>{"use strict";var t={917:t=>{t.exports=e}},s={};function o(e){var i=s[e];if(void 0!==i)return i.exports;var r=s[e]={exports:{}};return t[e](r,r.exports,o),r.exports}o.d=(e,t)=>{for(var s in t)o.o(t,s)&&!o.o(e,s)&&Object.defineProperty(e,s,{enumerable:!0,get:t[s]})},o.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),o.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})};var i={};return(()=>{o.r(i),o.d(i,{AudioNodeVAD:()=>A,FrameProcessor:()=>d,Message:()=>e,MicVAD:()=>b,NonRealTimeVAD:()=>T,defaultNonRealTimeVADOptions:()=>P,defaultRealTimeVADOptions:()=>y,utils:()=>R});var e,t=o(917);function s(e,t,s){for(var o=0;o<s.length;o++)e.setUint8(t+o,s.charCodeAt(o))}!function(e){e.AudioFrame="AUDIO_FRAME",e.SpeechStart="SPEECH_START",e.VADMisfire="VAD_MISFIRE",e.SpeechEnd="SPEECH_END",e.SpeechStop="SPEECH_STOP",e.AudioData="audio-data"}(e||(e={}));const r=["error","debug","warn"].reduce(((e,t)=>(e[t]=function(e){return(...t)=>{console[e]("[VAD]",...t)}}(t),e)),{}),n=[512,1024,1536],a={positiveSpeechThreshold:.5,negativeSpeechThreshold:.35,preSpeechPadFrames:1,redemptionFrames:8,frameSamples:1536,minSpeechFrames:3,submitUserSpeechOnPause:!1};function h(e){n.includes(e.frameSamples)||r.warn("You are using an unusual frame size"),(e.positiveSpeechThreshold<0||e.negativeSpeechThreshold>1)&&r.error("postiveSpeechThreshold should be a number between 0 and 1"),(e.negativeSpeechThreshold<0||e.negativeSpeechThreshold>e.positiveSpeechThreshold)&&r.error("negativeSpeechThreshold should be between 0 and postiveSpeechThreshold"),e.preSpeechPadFrames<0&&r.error("preSpeechPadFrames should be positive"),e.redemptionFrames<0&&r.error("preSpeechPadFrames should be positive")}const c=e=>{const t=e.reduce(((e,t)=>(e.push(e.at(-1)+t.length),e)),[0]),s=new Float32Array(t.at(-1));return e.forEach(((e,o)=>{const i=t[o];s.set(e,i)})),s};class d{constructor(t,s,o){this.modelProcessFunc=t,this.modelResetFunc=s,this.options=o,this.speaking=!1,this.redemptionCounter=0,this.active=!1,this.reset=()=>{this.speaking=!1,this.audioBuffer=[],this.modelResetFunc(),this.redemptionCounter=0},this.pause=()=>(this.active=!1,this.options.submitUserSpeechOnPause?this.endSegment():(this.reset(),{})),this.resume=()=>{this.active=!0},this.endSegment=()=>{const t=this.audioBuffer;this.audioBuffer=[];const s=this.speaking;this.reset();const o=t.reduce(((e,t)=>e+ +t.isSpeech),0);if(s){if(o>=this.options.minSpeechFrames){const s=c(t.map((e=>e.frame)));return{msg:e.SpeechEnd,audio:s}}return{msg:e.VADMisfire}}return{}},this.process=async t=>{if(!this.active)return{};const s=await this.modelProcessFunc(t);if(this.audioBuffer.push({frame:t,isSpeech:s.isSpeech>=this.options.positiveSpeechThreshold}),s.isSpeech>=this.options.positiveSpeechThreshold&&this.redemptionCounter&&(this.redemptionCounter=0),s.isSpeech>=this.options.positiveSpeechThreshold&&!this.speaking)return this.speaking=!0,{probs:s,msg:e.SpeechStart};if(s.isSpeech<this.options.negativeSpeechThreshold&&this.speaking&&++this.redemptionCounter>=this.options.redemptionFrames){this.redemptionCounter=0,this.speaking=!1;const t=this.audioBuffer;if(this.audioBuffer=[],t.reduce(((e,t)=>e+ +t.isSpeech),0)>=this.options.minSpeechFrames){const o=c(t.map((e=>e.frame)));return{probs:s,msg:e.SpeechEnd,audio:o}}return{probs:s,msg:e.VADMisfire}}if(!this.speaking)for(;this.audioBuffer.length>this.options.preSpeechPadFrames;)this.audioBuffer.shift();return{probs:s}},this.audioBuffer=[],this.reset()}}class p{constructor(e,t){this.ort=e,this.modelFetcher=t,this.init=async()=>{r.debug("initializing vad");const e=await this.modelFetcher();this._session=await this.ort.InferenceSession.create(e),this._sr=new this.ort.Tensor("int64",[16000n]),this.reset_state(),r.debug("vad is initialized")},this.reset_state=()=>{const e=Array(128).fill(0);this._h=new this.ort.Tensor("float32",e,[2,1,64]),this._c=new this.ort.Tensor("float32",e,[2,1,64])},this.process=async e=>{const t={input:new this.ort.Tensor("float32",e,[1,e.length]),h:this._h,c:this._c,sr:this._sr},s=await this._session.run(t);this._h=s.hn,this._c=s.cn;const[o]=s.output.data;return{notSpeech:1-o,isSpeech:o}}}}p.new=async(e,t)=>{const s=new p(e,t);return await s.init(),s};class u{constructor(e){this.options=e,this.process=e=>{const t=[];for(const t of e)this.inputBuffer.push(t);for(;this.inputBuffer.length*this.options.targetSampleRate/this.options.nativeSampleRate>this.options.targetFrameSize;){const e=new Float32Array(this.options.targetFrameSize);let s=0,o=0;for(;s<this.options.targetFrameSize;){let t=0,i=0;for(;o<Math.min(this.inputBuffer.length,(s+1)*this.options.nativeSampleRate/this.options.targetSampleRate);)t+=this.inputBuffer[o],i++,o++;e[s]=t/i,s++}this.inputBuffer=this.inputBuffer.slice(o),t.push(e)}return t},e.nativeSampleRate<16e3&&r.error("nativeSampleRate is too low. Should have 16000 = targetSampleRate <= nativeSampleRate"),this.inputBuffer=[]}}const l={...a,ortConfig:void 0};class m{static async _new(e,t,s={}){const o={...l,...s};void 0!==o.ortConfig&&o.ortConfig(t);const i=new this(e,t,o);return await i.init(),i}constructor(t,s,o){this.modelFetcher=t,this.ort=s,this.options=o,this.init=async()=>{const e=await p.new(this.ort,this.modelFetcher);this.frameProcessor=new d(e.process,e.reset_state,{frameSamples:this.options.frameSamples,positiveSpeechThreshold:this.options.positiveSpeechThreshold,negativeSpeechThreshold:this.options.negativeSpeechThreshold,redemptionFrames:this.options.redemptionFrames,preSpeechPadFrames:this.options.preSpeechPadFrames,minSpeechFrames:this.options.minSpeechFrames,submitUserSpeechOnPause:this.options.submitUserSpeechOnPause}),this.frameProcessor.resume()},this.run=async function*(t,s){const o={nativeSampleRate:s,targetSampleRate:16e3,targetFrameSize:this.options.frameSamples},i=new u(o).process(t);let r,n;for(const t of[...Array(i.length)].keys()){const s=i[t],{msg:o,audio:a}=await this.frameProcessor.process(s);switch(o){case e.SpeechStart:r=t*this.options.frameSamples/16;break;case e.SpeechEnd:n=(t+1)*this.options.frameSamples/16,yield{audio:a,start:r,end:n}}}const{msg:a,audio:h}=this.frameProcessor.endSegment();a==e.SpeechEnd&&(yield{audio:h,start:r,end:i.length*this.options.frameSamples/16})},h(o)}}const f={minFramesForTargetMS:function(e,t,s=16e3){return Math.ceil(e*s/1e3/t)},arrayBufferToBase64:function(e){for(var t="",s=new Uint8Array(e),o=s.byteLength,i=0;i<o;i++)t+=String.fromCharCode(s[i]);return btoa(t)},encodeWAV:function(e,t=3,o=16e3,i=1,r=32){var n=r/8,a=i*n,h=new ArrayBuffer(44+e.length*n),c=new DataView(h);return s(c,0,"RIFF"),c.setUint32(4,36+e.length*n,!0),s(c,8,"WAVE"),s(c,12,"fmt "),c.setUint32(16,16,!0),c.setUint16(20,t,!0),c.setUint16(22,i,!0),c.setUint32(24,o,!0),c.setUint32(28,o*a,!0),c.setUint16(32,a,!0),c.setUint16(34,r,!0),s(c,36,"data"),c.setUint32(40,e.length*n,!0),1===t?function(e,t,s){for(var o=0;o<s.length;o++,t+=2){var i=Math.max(-1,Math.min(1,s[o]));e.setInt16(t,i<0?32768*i:32767*i,!0)}}(c,44,e):function(e,t,s){for(var o=0;o<s.length;o++,t+=4)e.setFloat32(t,s[o],!0)}(c,44,e),h}},S=e=>fetch(e).then((e=>e.arrayBuffer())),g="undefined"!=typeof window&&void 0!==window.document?window.document.currentScript:null;let w="/";g&&(w=g.src.replace(/#.*$/,"").replace(/\?.*$/,"").replace(/\/[^\/]+$/,"/"));const v=e=>w+e,F=t,y={...a,onFrameProcessed:e=>{},onVADMisfire:()=>{r.debug("VAD misfire")},onSpeechStart:()=>{r.debug("Detected speech start")},onSpeechEnd:()=>{r.debug("Detected speech end")},onAudioFrame:e=>{console.log("Received an audio frame")},workletURL:v("vad.worklet.bundle.min.js"),modelURL:v("silero_vad.onnx"),modelFetcher:S,stream:void 0,ortConfig:void 0};class b{static async new(e={}){const t={...y,...e};let s;h(t),s=void 0===t.stream?await navigator.mediaDevices.getUserMedia({audio:{...t.additionalAudioConstraints,channelCount:1,echoCancellation:!0,autoGainControl:!0,noiseSuppression:!0,...t.additionalAudioConstraints?t.additionalAudioConstraints:{}}}):t.stream;const o=new AudioContext,i=new MediaStreamAudioSourceNode(o,{mediaStream:s}),r=await A.new(o,t);return r.receive(i),new b(t,o,s,r,i)}constructor(e,t,s,o,i,r=!1){this.options=e,this.audioContext=t,this.stream=s,this.audioNodeVAD=o,this.sourceNode=i,this.listening=r,this.pause=()=>{this.audioNodeVAD.pause(),this.listening=!1},this.start=()=>{this.audioNodeVAD.start(),this.listening=!0},this.destroy=()=>{this.listening&&this.pause(),void 0===this.options.stream&&this.stream.getTracks().forEach((e=>e.stop())),this.sourceNode.disconnect(),this.audioNodeVAD.destroy(),this.audioContext.close()}}}class A{static async new(t,s={}){const o={...y,...s};h(o),void 0!==o.ortConfig&&o.ortConfig(F);try{await t.audioWorklet.addModule(o.workletURL)}catch(e){throw console.error(`Encountered an error while loading worklet. Please make sure the worklet vad.bundle.min.js included with @ricky0123/vad-web is available at the specified path:\n        ${o.workletURL}\n        If need be, you can customize the worklet file location using the \`workletURL\` option.`),e}const i=new AudioWorkletNode(t,"vad-helper-worklet",{processorOptions:{frameSamples:o.frameSamples}});let r;try{r=await p.new(F,(()=>o.modelFetcher(o.modelURL)))}catch(e){throw console.error(`Encountered an error while loading model file. Please make sure silero_vad.onnx, included with @ricky0123/vad-web, is available at the specified path:\n      ${o.modelURL}\n      If need be, you can customize the model file location using the \`modelsURL\` option.`),e}const n=new d(r.process,r.reset_state,{frameSamples:o.frameSamples,positiveSpeechThreshold:o.positiveSpeechThreshold,negativeSpeechThreshold:o.negativeSpeechThreshold,redemptionFrames:o.redemptionFrames,preSpeechPadFrames:o.preSpeechPadFrames,minSpeechFrames:o.minSpeechFrames,submitUserSpeechOnPause:o.submitUserSpeechOnPause}),a=new A(t,o,n,i);return i.port.onmessage=async t=>{if(t.data?.message===e.AudioFrame){const e=t.data.data,s=new Float32Array(e);await a.processFrame(s)}},a}constructor(t,s,o,i){this.ctx=t,this.options=s,this.frameProcessor=o,this.entryNode=i,this.pause=()=>{const e=this.frameProcessor.pause();this.handleFrameProcessorEvent(e)},this.start=()=>{this.frameProcessor.resume()},this.receive=e=>{e.connect(this.entryNode)},this.processFrame=async e=>{const t=await this.frameProcessor.process(e);this.handleFrameProcessorEvent(t),this.options.onAudioFrame(e)},this.handleFrameProcessorEvent=t=>{switch(void 0!==t.probs&&this.options.onFrameProcessed(t.probs),t.msg){case e.SpeechStart:this.options.onSpeechStart();break;case e.VADMisfire:this.options.onVADMisfire();break;case e.SpeechEnd:this.options.onSpeechEnd(t.audio)}},this.destroy=()=>{this.entryNode.port.postMessage({message:e.SpeechStop}),this.entryNode.disconnect()}}}const P={modelURL:v("silero_vad.onnx"),modelFetcher:S};class T extends m{static async new(e={}){const{modelURL:s,modelFetcher:o}={...P,...e};return await this._new((()=>o(s)),t,e)}}const R={audioFileToArray:async function(e){const t=new OfflineAudioContext(1,1,44100),s=new FileReader;let o=null;if(await new Promise((i=>{s.addEventListener("loadend",(e=>{const r=s.result;t.decodeAudioData(r,(e=>{o=e,t.startRendering().then((e=>{console.log("Rendering completed successfully"),i()})).catch((e=>{console.error(`Rendering failed: ${e}`)}))}),(e=>{console.log(`Error with decoding audio data: ${e}`)}))})),s.readAsArrayBuffer(e)})),null===o)throw Error("some shit");let i=o,r=new Float32Array(i.length);for(let e=0;e<i.length;e++)for(let t=0;t<i.numberOfChannels;t++)r[e]+=i.getChannelData(t)[e];return{audio:r,sampleRate:i.sampleRate}},...f}})(),i})()));